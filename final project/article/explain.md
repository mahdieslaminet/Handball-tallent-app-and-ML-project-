# داستان پروژه با زبان خیلی ساده
ما یک فایل داده داریم (heart.csv) که توش اطلاعات چند صد نفر هست (سن، فشار خون، کلسترول، …). کنار این اطلاعات، یک ستون داریم به اسم target که می‌گه اون فرد بیماری قلبی دارد یا نه:
•	target = 1 یعنی بیمار هست  
•	target = 0 یعنی بیمار نیست  

هدف ما اینه:  
یک برنامه بسازیم که با دیدن اطلاعات یک نفر، حدس بزنه بیمار هست یا نه.  
این همون چیزیه که بهش می‌گن “یادگیری ماشین / Machine Learning”.

________________________________________

# اصطلاحات مهم (ترجمه + توضیح ساده)

## 1) Machine Learning (یادگیری ماشین)
یعنی به جای اینکه خودمون قانون دستی بنویسیم (مثلاً اگر سن بالا بود بیمار است)،  
به کامپیوتر مثال‌های واقعی می‌دیم تا خودش الگوها رو یاد بگیره و بعد روی آدم‌های جدید پیش‌بینی کنه.

## 2) Dataset / Data (دیتاست)
همون فایل داده‌هاست. اینجا:  
•	303 نفر (303 ردیف)  
•	13 ویژگی (13 ستون اطلاعاتی)  
•	1 ستون برچسب (target)

## 3) Feature (ویژگی)
هر ستون اطلاعاتی که درباره فرد داریم. مثل:  
•	age سن  
•	chol کلسترول  
•	thalach بیشترین ضربان قلب  
•	cp نوع درد قفسه سینه  
این‌ها می‌شن Feature.

## 4) Label / Target (برچسب/هدف)
ستونی که جواب درست را می‌گوید. اینجا target که می‌گوید بیمار هست یا نه.

## 5) Classification (طبقه‌بندی)
یعنی خروجی ما “دسته‌ای” است نه عدد دقیق. اینجا فقط دو دسته داریم:  
•	بیمار (1)  
•	غیر بیمار (0)  
پس اسمش می‌شود Binary Classification (طبقه‌بندی دوحالته).

## 6) Model (مدل)
مدل یعنی “مغز آماری” که از داده یاد می‌گیرد. ما چند مدل مختلف امتحان کردیم تا ببینیم کدام بهتر پیش‌بینی می‌کند.

## 7) Train / Test Split (تقسیم آموزش/آزمون)
برای اینکه تقلب نشود:  
•	یک بخش داده را می‌گذاریم برای آموزش (Train) تا مدل یاد بگیرد  
•	یک بخش را می‌گذاریم برای آزمون (Test) تا ببینیم چقدر خوب پیش‌بینی می‌کند روی داده‌ای که قبلاً ندیده  

ما این کار را با نسبت 80/20 انجام دادیم:  
•	242 نفر برای آموزش  
•	61 نفر برای تست

## 8) Stratify
یعنی هنگام تقسیم Train/Test کاری می‌کنیم که نسبت بیمار/غیر بیمار در هر دو بخش تقریباً مشابه باشد.  
این باعث می‌شود تقسیم “منصفانه” شود.

## 9) StandardScaler (اسکیل کردن / هم‌مقیاس‌سازی)
بعضی ستون‌ها عددهای بزرگ دارند (کلسترول مثلاً 200) بعضی کوچک (مثل oldpeak حدود 1-3).  
اگر مقیاس‌ها خیلی متفاوت باشد، بعضی مدل‌ها گیج می‌شوند.  
Scaler می‌آید اعداد را طوری تبدیل می‌کند که:  
•	همه ویژگی‌ها تقریباً هم‌مقیاس شوند  
•	مدل‌های مثل SVM/KNN/LogReg بهتر کار کنند

## 10) Pipeline
یعنی یک مسیر مرتب:  
•	اول Scaling  
•	بعد آموزش مدل  
این کار باعث می‌شود تمیز و درست و بدون اشتباه اجرا شود.

## 11) Cross-Validation (اعتبارسنجی چندتایی / 5-Fold)
این خیلی مهمه:  
به جای اینکه فقط یک بار Train/Test تقسیم کنیم (که ممکنه شانسی خوب/بد بشه)،  
کل داده را به 5 بخش تقسیم می‌کنیم و 5 بار آموزش/تست انجام می‌دهیم.  
بعد میانگین نتایج را می‌گیریم.  
این می‌گوید کدام مدل واقعاً پایدارتر و قابل اعتمادتر است.

________________________________________

# ما دقیقاً چه مدل‌هایی تست کردیم؟

## 1) Logistic Regression (لجستیک رگرشن)
مدل ساده و کلاسیک برای پیش‌بینی دوحالته.  
خیلی سبک است، سریع اجرا می‌شود، و معمولاً خروجی قابل توضیح دارد.

## 2) SVM (RBF)
مدلی که سعی می‌کند یک مرز هوشمند بین دو کلاس بکشد.  
RBF یعنی مدل اجازه دارد مرز پیچیده‌تری بسازد.

## 3) KNN (k=7)
خیلی ساده:  
برای یک فرد جدید، می‌رود نزدیک‌ترین 7 نفر در داده را پیدا می‌کند و می‌گوید اکثریت آنها بیمار بودند یا نه.

## 4) RandomForest
یک مدل قوی که از چندین درخت تصمیم تشکیل شده.  
هر درخت یک نظر می‌دهد، آخر سر رأی‌گیری می‌شود.

________________________________________

# خروجی‌ها و معنی‌شون (خیلی ساده)

## Accuracy (دقت کلی)
از کل پیش‌بینی‌ها، چند درصد درست بوده؟  
مثلاً Accuracy=0.80 یعنی 80٪ درست.

## Precision (دقت مثبت)
وقتی مدل گفت “بیمار است”، چند درصد واقعاً بیمار بودند؟

## Recall (یادآوری/پوشش)
از بین همه بیمارهای واقعی، مدل چند درصد را پیدا کرد؟

## F1-score
یک عدد ترکیبی که هم Precision را در نظر می‌گیرد هم Recall را.  
وقتی بخواهیم یک معیار “متعادل” داشته باشیم، F1 خیلی محبوب است.

________________________________________

# Confusion Matrix (ماتریس سردرگمی) یعنی چی؟
این یک جدول است که دقیقاً نشان می‌دهد مدل کجا اشتباه کرده:  
•	TN: درست گفته غیر بیمار  
•	TP: درست گفته بیمار  
•	FP: اشتباه گفته بیمار (ولی طرف سالم بوده)  
•	FN: اشتباه گفته سالم (ولی طرف بیمار بوده)  
این برای تحلیل خطاها خیلی مهم است.

________________________________________

# نتایج تو پروژه تو چطور بود؟ (خلاصه با معنی)

## روی Test Set (فقط یک تقسیم)
جدول مقایسه نشان داد:  
•	RandomForest بهترین F1 را داد (حدود 0.85)  
•	Accuracy هم برای چند مدل نزدیک 0.82 بود

## روی 5-Fold Cross-Validation (قابل اعتمادتر)
اینجا Logistic Regression بهترین شد:  
•	acc_mean ≈ 0.84  
•	f1_mean ≈ 0.86  

پس چی شد؟  
•	RandomForest روی یک تست خاص خیلی خوب زد  
•	ولی Logistic Regression به طور متوسط و پایدارتر بهتر بود  
این نتیجه خیلی خوبه چون به استاد نشون می‌ده تو فرق بین “یک بار تست” و “تست پایدار” رو می‌فهمی.

________________________________________

# عکس‌ها چی هستن و چی می‌گن؟

## 1) تصویر ROC Curve (roc_curves.png)
ROC یک نمودار است که نشان می‌دهد مدل چقدر خوب می‌تواند بین بیمار و سالم فرق بگذارد.  
•	محور افقی: False Positive Rate (چقدر اشتباه بیمار اعلام می‌کنیم)  
•	محور عمودی: True Positive Rate (چقدر درست بیمارها را پیدا می‌کنیم)  
هرچه منحنی بالاتر و چسبیده‌تر به گوشه بالا-چپ باشد، بهتر.

AUC یعنی چی؟  
AUC یک عدد بین 0.5 تا 1 است:  
•	0.5 یعنی مثل شیر یا خط (بد)  
•	0.7 خوب  
•	0.8 خیلی خوب  
•	نزدیک 0.9 عالی  

تو خروجی تو:  
•	RandomForest AUC ≈ 0.90 (بهترین)  
•	بقیه هم نزدیک 0.88 بودند که عالیه  
پس این عکس می‌گه: مدل‌ها واقعاً قدرت تشخیص خوبی دارند.

________________________________________

## 2) تصویر Feature Importance (feature_importance_rf.png)
این عکس فقط برای RandomForest است و می‌گوید:  
کدام ستون‌ها (ویژگی‌ها) در تصمیم‌گیری مدل مهم‌تر بودند.  

تو خروجی Top 8:  
1.	cp (نوع درد قفسه سینه) مهم‌ترین  
2.	thalach (حداکثر ضربان قلب)  
3.	thal  
4.	oldpeak  
…  
یعنی مدل بیشتر از همه روی این‌ها حساب کرده تا بفهمه بیماری هست یا نه.

________________________________________

# دقیقاً “چه کار کردیم” در یک خط
داده را گرفتیم → آموزش/تست کردیم → ۴ مدل را مقایسه کردیم → با Cross-Validation مطمئن شدیم پایدار است → ROC/AUC رسم کردیم → Feature Importance گرفتیم → نتایج را در گزارش و عکس‌ها گذاشتیم.
